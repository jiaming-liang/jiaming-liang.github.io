# jemdoc: menu{MENU}{teaching.html}
= Nonlinear Optimization, Spring 2025

This course serves as a modern introduction to the field of optimization. It covers important topics such as convexity, optimality conditions, duality, gradient methods, and Newtonâ€™s method. The objective is to provide the foundations of theory and algorithms of nonlinear optimization, as well as to present a variety of applications from diverse areas.

== Course Information

- [./teaching/NLOPT/syllabus.pdf Syllabus]

- Instructor: [https://jiaming-liang.github.io/ Jiaming Liang]

- Teaching Assistant: Youwei Wang

- Meeting Information: 2:00-3:15 pm, Tuesday/Thursday, Harkness Hall 210

- Office Hours
-- 4:00-5:00 pm, Wednesday, Wegmans Hall 2403 (Jiaming Liang)
-- 4:00-5:00 pm, Friday, Wegmans Hall 1219 (Youwei Wang)

- Textbooks
-- Amir Beck. /Introduction to Nonlinear Optimization, Second Edition/. SIAM, 2023.

- Recommended Readings
-- Dimitri P. Bertsekas. /Nonlinear Programming, Third Edition/. Athena Scientific, 2016.
-- Stephen Boyd and Lieven Vandenberghe. /Convex Optimization/. Cambridge University Press, 2004.
-- David G. Luenberger and Yinyu Ye. /Linear and Nonlinear Programming, Third Edition/. Springer, 2008.
-- Fatma Kilinc-Karzan and Arkadi Nemirovski. /Mathematical Essentials for Convex Optimization/. Cambridge University Press, 2024+.


#== Topics

#- Introduction
#-- Applications in ML [./teaching/OPTML/leture_1_introduction.pptx \[slides\]]\n
#-- Convexity and complexity [./teaching/OPTML/Lec2_Convexity_and_Complexity.pdf \[notes\]]\n

#- First-order methods I: basic concepts
#-- Gradient method [./teaching/OPTML/Lec3_Gradient_Method.pdf \[notes\]]\n
#-- Subgradient method [./teaching/OPTML/Lec4_Subgradient_Method.pdf \[notes\]]\n
#-- Mirror descent [./teaching/OPTML/Lec5_Mirror_Descent.pdf \[notes\]]\n
#-- Proximal gradient method [./teaching/OPTML/Lec6_Proximal_Gradient.pdf \[notes\]]\n
#-- Accelerated gradient method [./teaching/OPTML/Lec7_Accelerated_Gradient.pdf \[notes\]]\n
#-- Frank-Wolfe method [./teaching/OPTML/Lec8_Frank-Wolfe_Method.pdf \[notes\]]\n

#- First-order methods II: advanced topics
#-- Dual methods [./teaching/OPTML/Lec9_Dual_Methods.pdf \[notes\]]\n
#-- Stochastic approximation [./teaching/OPTML/Lec10_Stochastic_Approximation.pdf \[notes\]]\n
#-- Augmented Lagrangian [./teaching/OPTML/Lec10_Augmented_Lagrangian.pdf \[notes\]]\n
#-- Smoothing techniques [./teaching/OPTML/Lec11_Smoothing_Technique.pdf \[notes\]]\n
#-- Optimization in relative scale [./teaching/OPTML/Lec12_Optimization_in_Relative_Scale.pdf \[notes\]]\n
#-- Randomized block coordinate descent [./teaching/OPTML/Lec13_Randomized_Block_Coordinate_Descent.pdf \[notes\]]\n
#-- Universal method

#- Selected topics in machine learning
#-- Nonconvex optimization [./teaching/OPTML/Lec14_Nonconvex_Optimization.pdf \[notes\]]\n
#-- Distributed optimization
#-- Manifold optimization
#-- Reinforcement learning [./teaching/OPTML/Lec16_Reinforcement_Learning.pdf \[notes\]]\n

#- Beyond first-order methods
#-- Second and higher-order methods
