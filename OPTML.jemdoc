# jemdoc: menu{MENU}{teaching.html}
= Optimization for Machine Learning, Fall 2023

This course primarily focuses on algorithms for large-scale optimization problems arising in machine learning and data science applications. The first part will cover first-order methods including gradient and subgradient methods, mirror descent, proximal gradient method, accelerated gradient method, Frank-Wolfe method, and inexact proximal point methods. The second part will introduce algorithms for nonconvex optimization, stochastic optimization, distributed optimization, manifold optimization, reinforcement learning, and those beyond first-order.

== Course Information

- [./teaching/OPTML/syllabus.pdf Syllabus]

- Instructor: [https://jiaming-liang.github.io/ Jiaming Liang]

- Teaching Assistant: Lin Zang

- Meeting Information: 9:40-10:55 am, Tuesday/Thursday, Hylan Building 203

- Office Hours
-- 4:00-5:oo pm, Wednesday, Wegmans Hall 2403 (Jiaming Liang)
-- 4:00-5:oo pm, Friday, Wegmans Hall 1219 (Lin Zang)

- Textbooks
-- Amir Beck. /First-order methods in optimization/. SIAM, 2017.
-- Yurii Nesterov. /Lectures on convex optimization/. Springer, 2018.

- Recommended Readings
-- Guanghui Lan. /First-order and Stochastic Optimization Methods for Machine Learning/. Springer, 2020.
-- Benjamin Recht and Stephen Wright. /Optimization for Data Analysis/. Cambridge University Press, 2022.
-- Suvrit Sra, Sebastian Nowozin, and Stephen Wright, eds. /Optimization for Machine Learning/. MIT Press, 2011.

== Topics

- Introduction
-- Applications in ML [./teaching/OPTML/leture_1_introduction.pptx \[slides\]]\n
-- Convexity and complexity [./teaching/OPTML/Lec2_Convexity_and_Complexity.pdf \[notes\]]\n

- First-order methods I: basic concepts
-- Gradient method [./teaching/OPTML/Lec3_Gradient_Method.pdf \[notes\]]\n
-- Subgradient method [./teaching/OPTML/Lec4_Subgradient_Method.pdf \[notes\]]\n
-- Mirror descent [./teaching/OPTML/Lec5_Mirror_Descent.pdf \[notes\]]\n
-- Proximal gradient method [./teaching/OPTML/Lec6_Proximal_Gradient.pdf \[notes\]]\n
-- Accelerated gradient method [./teaching/OPTML/Lec7_Accelerated_Gradient.pdf \[notes\]]\n
-- Frank-Wolfe method [./teaching/OPTML/Lec8_Frank-Wolfe_Method.pdf \[notes\]]\n

- First-order methods II: advanced topics
-- Inexact proximal point methods
-- Operator splitting
-- Randomized coordinate descent
-- Optimization in relative scale
-- Smoothing techniques

- Selected topics in machine learning
-- Nonconvex optimization
-- Stochastic optimization
-- Distributed optimization
-- Manifold optimization
-- Reinforcement learning

- Beyond first-order methods
-- Second-order methods 
-- Higher-order methods
