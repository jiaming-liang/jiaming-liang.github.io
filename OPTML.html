<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="R.ico" />
<title>Optimization for Machine Learning, Fall 2024</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Optimization for Machine Learning, Fall 2024</h1>
</div>
<p>This course primarily focuses on algorithms for large-scale optimization problems arising in machine learning and data science applications. The first part will cover various first-order methods including gradient and subgradient methods, mirror descent, proximal gradient method, accelerated gradient method, Frank-Wolfe method, and dual methods. The second part will survey topics in machine learning from an optimization perspective, e.g., stochastic optimization, distributionally robust optimization, online learning, and reinforcement learning.</p>
<h2>Course Information</h2>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=I_6bIrbaBW0&amp;list=PLuJY91x7h5orCtyh6mChurVQ2ZZef9qPF&amp;ab_channel=JiamingLiang">YouTube videos from Fall 2023</a></p>
</li>
</ul>
<ul>
<li><p><a href="./teaching/OPTML/syllabus.pdf">Syllabus</a></p>
</li>
</ul>
<ul>
<li><p>Instructor: <a href="https://jiaming-liang.github.io/">Jiaming Liang</a></p>
</li>
</ul>
<ul>
<li><p>Teaching Assistant: Lin Zang</p>
</li>
</ul>
<ul>
<li><p>Meeting Information: 9:40-10:55 am, Tuesday/Thursday, Hylan Building 203</p>
</li>
</ul>
<ul>
<li><p>Office Hours</p>
<ul>
<li><p>4:00-5:00 pm, Wednesday, Wegmans Hall 2403 (Jiaming Liang)</p>
</li>
<li><p>4:00-5:00 pm, Friday, Wegmans Hall 1219 (Lin Zang)</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Textbooks</p>
<ul>
<li><p>Amir Beck. <i>First-order methods in optimization</i>. SIAM, 2017.</p>
</li>
<li><p>Yurii Nesterov. <i>Lectures on convex optimization</i>. Springer, 2018.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Recommended Readings</p>
<ul>
<li><p>Fatma Kilinc-Karzan and Arkadi Nemirovski. <i>Mathematical Essentials for Convex Optimization</i>. Cambridge University Press, 2024+.</p>
</li>
<li><p>Guanghui Lan. <i>First-order and Stochastic Optimization Methods for Machine Learning</i>. Springer, 2020.</p>
</li>
<li><p>Benjamin Recht and Stephen Wright. <i>Optimization for Data Analysis</i>. Cambridge University Press, 2022.</p>
</li>
<li><p>Suvrit Sra, Sebastian Nowozin, and Stephen Wright, eds. <i>Optimization for Machine Learning</i>. MIT Press, 2011.</p>
</li>
</ul>

</li>
</ul>
<h2>Topics</h2>
<ul>
<li><p>Introduction</p>
<ul>
<li><p>Applications in ML <a href="./teaching/OPTML/leture_1_introduction.pptx">[slides]</a><br /></p>
</li>
<li><p>Convexity and complexity <a href="./teaching/OPTML/Lec2_Convexity_and_Complexity.pdf">[notes]</a><br /></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>First-order methods I: basic concepts</p>
<ul>
<li><p>Gradient method <a href="./teaching/OPTML/Lec3_Gradient_Method.pdf">[notes]</a><br /></p>
</li>
<li><p>Subgradient method <a href="./teaching/OPTML/Lec4_Subgradient_Method.pdf">[notes]</a><br /></p>
</li>
<li><p>Mirror descent <a href="./teaching/OPTML/Lec5_Mirror_Descent.pdf">[notes]</a><br /></p>
</li>
<li><p>Proximal gradient method <a href="./teaching/OPTML/Lec6_Proximal_Gradient.pdf">[notes]</a><br /></p>
</li>
<li><p>Accelerated gradient method <a href="./teaching/OPTML/Lec7_Accelerated_Gradient.pdf">[notes]</a><br /></p>
</li>
<li><p>Frank-Wolfe method <a href="./teaching/OPTML/Lec8_Frank-Wolfe_Method.pdf">[notes]</a><br /></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>First-order methods II: advanced topics</p>
<ul>
<li><p>Dual methods <a href="./teaching/OPTML/Lec9_Dual_Methods.pdf">[notes]</a><br /></p>
</li>
<li><p>Stochastic approximation <a href="./teaching/OPTML/Lec10_Stochastic_Approximation.pdf">[notes]</a><br /></p>
</li>
<li><p>Smoothing techniques <a href="./teaching/OPTML/Lec11_Smoothing_Techniques.pdf">[notes]</a><br />



</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-11-13 23:38:58 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
