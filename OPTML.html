<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="R.ico" />
<title>Optimization for Machine Learning, Fall 2023</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Optimization for Machine Learning, Fall 2023</h1>
</div>
<p>This course primarily focuses on algorithms for large-scale optimization problems arising in machine learning and data science applications. The first part will cover first-order methods including gradient and subgradient methods, mirror descent, proximal gradient method, accelerated gradient method, Frank-Wolfe method, and inexact proximal point methods. The second part will introduce algorithms for nonconvex optimization, stochastic optimization, distributed optimization, manifold optimization, reinforcement learning, and those beyond first-order.</p>
<h2>Course Information</h2>
<ul>
<li><p><a href="./teaching/OPTML/syllabus.pdf">Syllabus</a></p>
</li>
</ul>
<ul>
<li><p>Instructor: <a href="https://jiaming-liang.github.io/">Jiaming Liang</a></p>
</li>
</ul>
<ul>
<li><p>Teaching Assistant: Lin Zang</p>
</li>
</ul>
<ul>
<li><p>Meeting Information: 9:40-10:55 am, Tuesday/Thursday, Hylan Building 203</p>
</li>
</ul>
<ul>
<li><p>Office Hours</p>
<ul>
<li><p>4:00-5:00 pm, Wednesday, Wegmans Hall 2403 (Jiaming Liang)</p>
</li>
<li><p>4:00-5:00 pm, Friday, Wegmans Hall 1219 (Lin Zang)</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Textbooks</p>
<ul>
<li><p>Amir Beck. <i>First-order methods in optimization</i>. SIAM, 2017.</p>
</li>
<li><p>Yurii Nesterov. <i>Lectures on convex optimization</i>. Springer, 2018.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Recommended Readings</p>
<ul>
<li><p>Guanghui Lan. <i>First-order and Stochastic Optimization Methods for Machine Learning</i>. Springer, 2020.</p>
</li>
<li><p>Benjamin Recht and Stephen Wright. <i>Optimization for Data Analysis</i>. Cambridge University Press, 2022.</p>
</li>
<li><p>Suvrit Sra, Sebastian Nowozin, and Stephen Wright, eds. <i>Optimization for Machine Learning</i>. MIT Press, 2011.</p>
</li>
</ul>

</li>
</ul>
<h2>Topics</h2>
<ul>
<li><p>Introduction</p>
<ul>
<li><p>Applications in ML <a href="./teaching/OPTML/leture_1_introduction.pptx">[slides]</a><br /></p>
</li>
<li><p>Convexity and complexity <a href="./teaching/OPTML/Lec2_Convexity_and_Complexity.pdf">[notes]</a><br /></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>First-order methods I: basic concepts</p>
<ul>
<li><p>Gradient method <a href="./teaching/OPTML/Lec3_Gradient_Method.pdf">[notes]</a><br /></p>
</li>
<li><p>Subgradient method <a href="./teaching/OPTML/Lec4_Subgradient_Method.pdf">[notes]</a><br /></p>
</li>
<li><p>Mirror descent <a href="./teaching/OPTML/Lec5_Mirror_Descent.pdf">[notes]</a><br /></p>
</li>
<li><p>Proximal gradient method <a href="./teaching/OPTML/Lec6_Proximal_Gradient.pdf">[notes]</a><br /></p>
</li>
<li><p>Accelerated gradient method <a href="./teaching/OPTML/Lec7_Accelerated_Gradient.pdf">[notes]</a><br /></p>
</li>
<li><p>Frank-Wolfe method <a href="./teaching/OPTML/Lec8_Frank-Wolfe_Method.pdf">[notes]</a><br /></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>First-order methods II: advanced topics</p>
<ul>
<li><p>Inexact proximal point methods <a href="./teaching/OPTML/Lec9_Inexact_Proximal_Point.pdf">[notes]</a><br /></p>
</li>
<li><p>Augmented Lagrangian <a href="./teaching/OPTML/Lec10_Augmented_Lagrangian.pdf">[notes]</a><br /></p>
</li>
<li><p>Smoothing techniques <a href="./teaching/OPTML/Lec11_Smoothing_Technique.pdf">[notes]</a><br /></p>
</li>
<li><p>Optimization in relative scale <a href="./teaching/OPTML/Lec12_Optimization_in_Relative_Scale.pdf">[notes]</a><br /></p>
</li>
<li><p>Randomized block coordinate descent <a href="./teaching/OPTML/Lec13_Randomized_Block_Coordinate_Descent.pdf">[notes]</a><br />
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Selected topics in machine learning</p>
<ul>
<li><p>Nonconvex optimization</p>
</li>
<li><p>Stochastic optimization

</p>
</li>
<li><p>Reinforcement learning</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Beyond first-order methods</p>
<ul>
<li><p>Second-order methods </p>
</li>
<li><p>Higher-order methods</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-11-21 13:38:16 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
